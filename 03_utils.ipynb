{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import subprocess\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "from PIL import ImageFilter, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class GaussianBlur(object):\n",
    "    \"\"\"\n",
    "    Apply Gaussian Blur to the PIL image.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n",
    "        self.prob = p\n",
    "        self.radius_min = radius_min\n",
    "        self.radius_max = radius_max\n",
    "\n",
    "    def __call__(self, img):\n",
    "        do_it = random.random() <= self.prob\n",
    "        if not do_it:\n",
    "            return img\n",
    "\n",
    "        return img.filter(\n",
    "            ImageFilter.GaussianBlur(\n",
    "                radius=random.uniform(self.radius_min, self.radius_max)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    \"\"\"\n",
    "    Apply Solarization to the PIL image.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size):\n",
    "    if os.path.isfile(pretrained_weights):\n",
    "        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "        if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "            state_dict = state_dict[checkpoint_key]\n",
    "        # remove `module.` prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n",
    "#     else:\n",
    "#         print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n",
    "#         url = None\n",
    "#         if model_name == \"vit_small\" and patch_size == 16:\n",
    "#             url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "#         elif model_name == \"vit_small\" and patch_size == 8:\n",
    "#             url = \"dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\"\n",
    "#         elif model_name == \"vit_base\" and patch_size == 16:\n",
    "#             url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "#         elif model_name == \"vit_base\" and patch_size == 8:\n",
    "#             url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "#         elif model_name == \"xcit_small_12_p16\":\n",
    "#             url = \"dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth\"\n",
    "#         elif model_name == \"xcit_small_12_p8\":\n",
    "#             url = \"dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth\"\n",
    "#         elif model_name == \"xcit_medium_24_p16\":\n",
    "#             url = \"dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pretrain.pth\"\n",
    "#         elif model_name == \"xcit_medium_24_p8\":\n",
    "#             url = \"dino_xcit_medium_24_p8_pretrain/dino_xcit_medium_24_p8_pretrain.pth\"\n",
    "#         elif model_name == \"resnet50\":\n",
    "#             url = \"dino_resnet50_pretrain/dino_resnet50_pretrain.pth\"\n",
    "#         if url is not None:\n",
    "#             print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n",
    "#             state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "#             model.load_state_dict(state_dict, strict=True)\n",
    "    else:\n",
    "        print(\"There is no reference weights available for this model => We use random weights.\")\n",
    "\n",
    "\n",
    "def load_pretrained_linear_weights(linear_classifier, model_name, patch_size):\n",
    "    url = None\n",
    "    if model_name == \"vit_small\" and patch_size == 16:\n",
    "        url = \"dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth\"\n",
    "    elif model_name == \"vit_small\" and patch_size == 8:\n",
    "        url = \"dino_deitsmall8_pretrain/dino_deitsmall8_linearweights.pth\"\n",
    "    elif model_name == \"vit_base\" and patch_size == 16:\n",
    "        url = \"dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth\"\n",
    "    elif model_name == \"vit_base\" and patch_size == 8:\n",
    "        url = \"dino_vitbase8_pretrain/dino_vitbase8_linearweights.pth\"\n",
    "    elif model_name == \"resnet50\":\n",
    "        url = \"dino_resnet50_pretrain/dino_resnet50_linearweights.pth\"\n",
    "    if url is not None:\n",
    "        print(\"We load the reference pretrained linear weights.\")\n",
    "        state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)[\"state_dict\"]\n",
    "        linear_classifier.load_state_dict(state_dict, strict=True)\n",
    "    else:\n",
    "        print(\"We use random linear weights.\")\n",
    "\n",
    "\n",
    "def clip_gradients(model, clip):\n",
    "    norms = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            norms.append(param_norm.item())\n",
    "            clip_coef = clip / (param_norm + 1e-6)\n",
    "            if clip_coef < 1:\n",
    "                p.grad.data.mul_(clip_coef)\n",
    "    return norms\n",
    "\n",
    "\n",
    "def cancel_gradients_last_layer(epoch, model, freeze_last_layer):\n",
    "    if epoch >= freeze_last_layer:\n",
    "        return\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"last_layer\" in n:\n",
    "            p.grad = None\n",
    "\n",
    "\n",
    "def restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Re-start from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(ckp_path):\n",
    "        return\n",
    "    print(\"Found checkpoint at {}\".format(ckp_path))\n",
    "\n",
    "    # open checkpoint file\n",
    "    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n",
    "\n",
    "    # key is what to look for in the checkpoint file\n",
    "    # value is the object to load\n",
    "    # example: {'state_dict': model}\n",
    "    for key, value in kwargs.items():\n",
    "        if key in checkpoint and value is not None:\n",
    "            try:\n",
    "                msg = value.load_state_dict(checkpoint[key], strict=False)\n",
    "                print(\"=> loaded '{}' from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    msg = value.load_state_dict(checkpoint[key])\n",
    "                    print(\"=> loaded '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "                except ValueError:\n",
    "                    print(\"=> failed to load '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "        else:\n",
    "            print(\"=> key '{}' not found in checkpoint: '{}'\".format(key, ckp_path))\n",
    "\n",
    "    # re load variable important for the run\n",
    "    if run_variables is not None:\n",
    "        for var_name in run_variables:\n",
    "            if var_name in checkpoint:\n",
    "                run_variables[var_name] = checkpoint[var_name]\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "\n",
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.\n",
    "    \"\"\"\n",
    "    FALSY_STRINGS = {\"off\", \"false\", \"0\"}\n",
    "    TRUTHY_STRINGS = {\"on\", \"true\", \"1\"}\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"invalid value for a boolean flag\")\n",
    "\n",
    "\n",
    "def fix_random_seeds(seed=31):\n",
    "    \"\"\"\n",
    "    Fix random seeds.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.6f} ({global_avg:.6f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.6f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def get_sha():\n",
    "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    def _run(command):\n",
    "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
    "    sha = 'N/A'\n",
    "    diff = \"clean\"\n",
    "    branch = 'N/A'\n",
    "    try:\n",
    "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
    "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
    "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
    "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
    "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    # launched with torch.distributed.launch\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    # launched with submitit on a slurm cluster\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    # launched naively with `python main_dino.py`\n",
    "    # we manually add MASTER_ADDR and MASTER_PORT to env variables\n",
    "    elif torch.cuda.is_available():\n",
    "        print('Will run the code on one GPU.')\n",
    "        args.rank, args.gpu, args.world_size = 0, 0, 1\n",
    "        os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "        os.environ['MASTER_PORT'] = '29500'\n",
    "    else:\n",
    "        print('Does not support training without GPU.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=args.dist_url,\n",
    "        world_size=args.world_size,\n",
    "        rank=args.rank,\n",
    "    )\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    print('| distributed init (rank {}): {}'.format(\n",
    "        args.rank, args.dist_url), flush=True)\n",
    "    dist.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    return [correct[:k].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "class LARS(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Almost copy-paste from https://github.com/facebookresearch/barlowtwins/blob/main/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, eta=0.001,\n",
    "                 weight_decay_filter=None, lars_adaptation_filter=None):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n",
    "                        eta=eta, weight_decay_filter=weight_decay_filter,\n",
    "                        lars_adaptation_filter=lars_adaptation_filter)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for g in self.param_groups:\n",
    "            for p in g['params']:\n",
    "                dp = p.grad\n",
    "\n",
    "                if dp is None:\n",
    "                    continue\n",
    "\n",
    "                if p.ndim != 1:\n",
    "                    dp = dp.add(p, alpha=g['weight_decay'])\n",
    "\n",
    "                if p.ndim != 1:\n",
    "                    param_norm = torch.norm(p)\n",
    "                    update_norm = torch.norm(dp)\n",
    "                    one = torch.ones_like(param_norm)\n",
    "                    q = torch.where(param_norm > 0.,\n",
    "                                    torch.where(update_norm > 0,\n",
    "                                                (g['eta'] * param_norm / update_norm), one), one)\n",
    "                    dp = dp.mul(q)\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'mu' not in param_state:\n",
    "                    param_state['mu'] = torch.zeros_like(p)\n",
    "                mu = param_state['mu']\n",
    "                mu.mul_(g['momentum']).add_(dp)\n",
    "\n",
    "                p.add_(mu, alpha=-g['lr'])\n",
    "\n",
    "\n",
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform forward pass separately on each resolution input.\n",
    "    The inputs corresponding to a single resolution are clubbed and single\n",
    "    forward is run on the same resolution inputs. Hence we do several\n",
    "    forward passes = number of different resolutions used. We then\n",
    "    concatenate all the output features and run the head forward on these\n",
    "    concatenated features.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, head):\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        # disable layers dedicated to ImageNet labels classification\n",
    "        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in x]),\n",
    "            return_counts=True,\n",
    "        )[1], 0)\n",
    "        start_idx, output = 0, torch.empty(0).to(x[0].device)\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.backbone(torch.cat(x[start_idx: end_idx]))\n",
    "            # The output is a tuple with XCiT model. See:\n",
    "            # https://github.com/facebookresearch/xcit/blob/master/xcit.py#L404-L405\n",
    "            if isinstance(_out, tuple):\n",
    "                _out = _out[0]\n",
    "            # accumulate outputs\n",
    "            output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "        # Run the head forward on the concatenated features.\n",
    "        return self.head(output)\n",
    "\n",
    "\n",
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n",
    "\n",
    "\n",
    "def has_batchnorms(model):\n",
    "    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bn_types):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class PCA():\n",
    "    \"\"\"\n",
    "    Class to  compute and apply PCA.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=256, whit=0.5):\n",
    "        self.dim = dim\n",
    "        self.whit = whit\n",
    "        self.mean = None\n",
    "\n",
    "    def train_pca(self, cov):\n",
    "        \"\"\"\n",
    "        Takes a covariance matrix (np.ndarray) as input.\n",
    "        \"\"\"\n",
    "        d, v = np.linalg.eigh(cov)\n",
    "        eps = d.max() * 1e-5\n",
    "        n_0 = (d < eps).sum()\n",
    "        if n_0 > 0:\n",
    "            d[d < eps] = eps\n",
    "\n",
    "        # total energy\n",
    "        totenergy = d.sum()\n",
    "\n",
    "        # sort eigenvectors with eigenvalues order\n",
    "        idx = np.argsort(d)[::-1][:self.dim]\n",
    "        d = d[idx]\n",
    "        v = v[:, idx]\n",
    "\n",
    "        print(\"keeping %.2f %% of the energy\" % (d.sum() / totenergy * 100.0))\n",
    "\n",
    "        # for the whitening\n",
    "        d = np.diag(1. / d**self.whit)\n",
    "\n",
    "        # principal components\n",
    "        self.dvt = np.dot(d, v.T)\n",
    "\n",
    "    def apply(self, x):\n",
    "        # input is from numpy\n",
    "        if isinstance(x, np.ndarray):\n",
    "            if self.mean is not None:\n",
    "                x -= self.mean\n",
    "            return np.dot(self.dvt, x.T).T\n",
    "\n",
    "        # input is from torch and is on GPU\n",
    "        if x.is_cuda:\n",
    "            if self.mean is not None:\n",
    "                x -= torch.cuda.FloatTensor(self.mean)\n",
    "            return torch.mm(torch.cuda.FloatTensor(self.dvt), x.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # input if from torch, on CPU\n",
    "        if self.mean is not None:\n",
    "            x -= torch.FloatTensor(self.mean)\n",
    "        return torch.mm(torch.FloatTensor(self.dvt), x.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "\n",
    "def compute_ap(ranks, nres):\n",
    "    \"\"\"\n",
    "    Computes average precision for given ranked indexes.\n",
    "    Arguments\n",
    "    ---------\n",
    "    ranks : zerro-based ranks of positive images\n",
    "    nres  : number of positive images\n",
    "    Returns\n",
    "    -------\n",
    "    ap    : average precision\n",
    "    \"\"\"\n",
    "\n",
    "    # number of images ranked by the system\n",
    "    nimgranks = len(ranks)\n",
    "\n",
    "    # accumulate trapezoids in PR-plot\n",
    "    ap = 0\n",
    "\n",
    "    recall_step = 1. / nres\n",
    "\n",
    "    for j in np.arange(nimgranks):\n",
    "        rank = ranks[j]\n",
    "\n",
    "        if rank == 0:\n",
    "            precision_0 = 1.\n",
    "        else:\n",
    "            precision_0 = float(j) / rank\n",
    "\n",
    "        precision_1 = float(j + 1) / (rank + 1)\n",
    "\n",
    "        ap += (precision_0 + precision_1) * recall_step / 2.\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "def compute_map(ranks, gnd, kappas=[]):\n",
    "    \"\"\"\n",
    "    Computes the mAP for a given set of returned results.\n",
    "         Usage:\n",
    "           map = compute_map (ranks, gnd)\n",
    "                 computes mean average precsion (map) only\n",
    "           map, aps, pr, prs = compute_map (ranks, gnd, kappas)\n",
    "                 computes mean average precision (map), average precision (aps) for each query\n",
    "                 computes mean precision at kappas (pr), precision at kappas (prs) for each query\n",
    "         Notes:\n",
    "         1) ranks starts from 0, ranks.shape = db_size X #queries\n",
    "         2) The junk results (e.g., the query itself) should be declared in the gnd stuct array\n",
    "         3) If there are no positive images for some query, that query is excluded from the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    map = 0.\n",
    "    nq = len(gnd) # number of queries\n",
    "    aps = np.zeros(nq)\n",
    "    pr = np.zeros(len(kappas))\n",
    "    prs = np.zeros((nq, len(kappas)))\n",
    "    nempty = 0\n",
    "\n",
    "    for i in np.arange(nq):\n",
    "        qgnd = np.array(gnd[i]['ok'])\n",
    "\n",
    "        # no positive images, skip from the average\n",
    "        if qgnd.shape[0] == 0:\n",
    "            aps[i] = float('nan')\n",
    "            prs[i, :] = float('nan')\n",
    "            nempty += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            qgndj = np.array(gnd[i]['junk'])\n",
    "        except:\n",
    "            qgndj = np.empty(0)\n",
    "\n",
    "        # sorted positions of positive and junk images (0 based)\n",
    "        pos  = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgnd)]\n",
    "        junk = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgndj)]\n",
    "\n",
    "        k = 0;\n",
    "        ij = 0;\n",
    "        if len(junk):\n",
    "            # decrease positions of positives based on the number of\n",
    "            # junk images appearing before them\n",
    "            ip = 0\n",
    "            while (ip < len(pos)):\n",
    "                while (ij < len(junk) and pos[ip] > junk[ij]):\n",
    "                    k += 1\n",
    "                    ij += 1\n",
    "                pos[ip] = pos[ip] - k\n",
    "                ip += 1\n",
    "\n",
    "        # compute ap\n",
    "        ap = compute_ap(pos, len(qgnd))\n",
    "        map = map + ap\n",
    "        aps[i] = ap\n",
    "\n",
    "        # compute precision @ k\n",
    "        pos += 1 # get it to 1-based\n",
    "        for j in np.arange(len(kappas)):\n",
    "            kq = min(max(pos), kappas[j]); \n",
    "            prs[i, j] = (pos <= kq).sum() / kq\n",
    "        pr = pr + prs[i, :]\n",
    "\n",
    "    map = map / (nq - nempty)\n",
    "    pr = pr / (nq - nempty)\n",
    "\n",
    "    return map, aps, pr, prs\n",
    "\n",
    "\n",
    "def multi_scale(samples, model):\n",
    "    v = None\n",
    "    for s in [1, 1/2**(1/2), 1/2]:  # we use 3 different scales\n",
    "        if s == 1:\n",
    "            inp = samples.clone()\n",
    "        else:\n",
    "            inp = nn.functional.interpolate(samples, scale_factor=s, mode='bilinear', align_corners=False)\n",
    "        feats = model(inp).clone()\n",
    "        if v is None:\n",
    "            v = feats\n",
    "        else:\n",
    "            v += feats\n",
    "    v /= 3\n",
    "    v /= v.norm()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script(fname='./03_utils.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
